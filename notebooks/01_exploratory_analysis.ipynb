{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploratory Analysis & Complexity Assessment\n",
    "\n",
    "This notebook provides a comprehensive analysis of the **Iris**, **Wine**, and **Breast Cancer** datasets. \n",
    "The goal is to understand the feature distributions, correlations, and inherent classification complexity of each dataset before applying Genetic Algorithm-optimized Decision Trees.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Data Loading](#1.-Setup-and-Data-Loading)\n",
    "2. [Feature Distributions & Class Balance](#2.-Feature-Distributions-&-Class-Balance)\n",
    "3. [Correlation Analysis](#3.-Correlation-Analysis)\n",
    "4. [Complexity Visualization (PCA + Decision Boundaries)](#4.-Complexity-Visualization)\n",
    "5. [Comparative Conclusion](#5.-Comparative-Conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris, load_wine, load_breast_cancer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Set Plotting Style\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\", palette=\"muted\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "We use a helper function to load datasets into Pandas DataFrames for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_as_df(loader_func, name):\n",
    "    data = loader_func()\n",
    "    df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "    df['target'] = data.target\n",
    "    # Map target indices to names if available\n",
    "    if hasattr(data, 'target_names'):\n",
    "        df['class'] = df['target'].map(lambda x: data.target_names[x])\n",
    "    else:\n",
    "        df['class'] = df['target'].astype(str)\n",
    "    return df, data, name\n",
    "\n",
    "# Load all 3 datasets\n",
    "iris_df, iris_data, iris_name = load_dataset_as_df(load_iris, \"Iris\")\n",
    "wine_df, wine_data, wine_name = load_dataset_as_df(load_wine, \"Wine\")\n",
    "cancer_df, cancer_data, cancer_name = load_dataset_as_df(load_breast_cancer, \"Breast Cancer\")\n",
    "\n",
    "datasets = [\n",
    "    (iris_df, iris_name),\n",
    "    (wine_df, wine_name),\n",
    "    (cancer_df, cancer_name)\n",
    "]\n",
    "\n",
    "for df, name in datasets:\n",
    "    print(f\"{name}: {df.shape[0]} samples, {df.shape[1]-2} features, {df['target'].nunique()} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Distributions & Class Balance\n",
    "\n",
    "Understanding class balance is critical. Imbalanced datasets (like Breast Cancer might be) can bias models toward the majority class.\n",
    "Boxplots help us identify features with different scales or outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (df, name) in enumerate(datasets):\n",
    "    sns.countplot(data=df, x='class', ax=axes[idx])\n",
    "    axes[idx].set_title(f\"{name} Class Distribution\")\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations:**\n",
    "> - **Iris**: Perfectly balanced (50 samples per class).\n",
    "> - **Wine**: Imbalanced. Class 1 (index 1) has more samples than Class 0 or 2.\n",
    "> - **Breast Cancer**: Significant imbalance. 'Benign' cases outnumber 'Malignant' cases roughly 2:1.\n",
    "\n",
    "### Feature Scales (Boxplots)\n",
    "We investigate the scale of features. Large disparities in scale (common in Wine and Cancer) suggest that normalization (StandardScaler) is important for distance-based methods or PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distributions(df, name):\n",
    "    # Select only numeric feature columns\n",
    "    feature_cols = [c for c in df.columns if c not in ['target', 'class']]\n",
    "    \n",
    "    # Normalize for visualization if scales are wildly different\n",
    "    df_std = pd.DataFrame(StandardScaler().fit_transform(df[feature_cols]), columns=feature_cols)\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    sns.boxplot(data=df_std)\n",
    "    plt.title(f\"{name} Feature Distributions (Standardized)\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "plot_distributions(wine_df, \"Wine\")\n",
    "plot_distributions(cancer_df, \"Breast Cancer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correlation Analysis\n",
    "\n",
    "Highly correlated features provide redundant information. Decision trees can handle colinearity well, but understanding it helps us interpret which features might be interchangeable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation(df, name):\n",
    "    feature_cols = [c for c in df.columns if c not in ['target', 'class']]\n",
    "    corr = df[feature_cols].corr()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr, cmap='coolwarm', center=0, annot=False, fmt='.1f')\n",
    "    plt.title(f\"{name} Correlation Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "plot_correlation(iris_df, \"Iris\")\n",
    "plot_correlation(wine_df, \"Wine\")\n",
    "plot_correlation(cancer_df, \"Breast Cancer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations:**\n",
    "> - **Iris**: Petal Length and Petal Width are extremely highly correlated (> 0.95), suggesting either one alone is a strong predictor.\n",
    "> - **Breast Cancer**: Many features (radius, perimeter, area) are geometrically related and thus highly correlated. The dataset has high redundancy.\n",
    "\n",
    "## 4. Complexity Visualization (PCA + Decision Boundaries)\n",
    "\n",
    "To visualize how \"hard\" these classification problems are, we project the high-dimensional data into **2D** using PCA.\n",
    "Then, we fit a simple Decision Tree to this 2D projection to visualize the decision boundaries.\n",
    "\n",
    "- **Simpler boundaries** = Easier dataset.\n",
    "- **Fragmented/Complex boundaries** = Harder dataset (more overlap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(df, name, ax):\n",
    "    # 1. Prepare Data\n",
    "    X = df.drop(columns=['target', 'class']).values\n",
    "    y = df['target'].values\n",
    "    \n",
    "    # 2. PCA Reduction to 2D\n",
    "    X_std = StandardScaler().fit_transform(X)\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_std)\n",
    "    \n",
    "    # 3. Train a Proxy Tree on the 2D data\n",
    "    # We aim for a somewhat detailed tree (depth=5) to show where it struggles\n",
    "    clf = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "    clf.fit(X_pca, y)\n",
    "    \n",
    "    # 4. Create Meshgrid\n",
    "    x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1\n",
    "    y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05),\n",
    "                         np.arange(y_min, y_max, 0.05))\n",
    "    \n",
    "    # 5. Predict\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # 6. Plot\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=40)\n",
    "    ax.set_title(f\"{name} (PCA 2D Projection)\\nExplained Var: {pca.explained_variance_ratio_.sum():.2f}\")\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "plot_decision_boundary(iris_df, \"Iris\", axes[0])\n",
    "plot_decision_boundary(wine_df, \"Wine\", axes[1])\n",
    "plot_decision_boundary(cancer_df, \"Breast Cancer\", axes[2])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparative Conclusion\n",
    "\n",
    "| Dataset | Samples | Features | Classes | Difficulty | Modeling Implications |\n",
    "|---------|---------|----------|---------|------------|-----------------------|\n",
    "| **Iris** | 150 | 4 | 3 | **Low** | Almost linearly separable. A very shallow tree (depth 2-3) should achieve >95% accuracy. Good for sanity checking the GA. |\n",
    "| **Wine** | 178 | 13 | 3 | **Medium** | Classes are fairly distinct in PCA space, but higher dimensionality (13 features) increases search space for the GA. |\n",
    "| **Cancer** | 569 | 30 | 2 | **High** | High dimensionality (30 features) and some overlap between benign/malignant classes in projection. Requires effective feature selection (which GA Trees do inherently). |\n",
    "\n",
    "### Implications for GA-Tree Optimization\n",
    "1.  **Search Space**: For Breast Cancer, the mutation operator needs to explore 30 possible feature splits, requiring a larger population or more generations compared to Iris.\n",
    "2.  **Regularization**: The \"High\" difficulty datasets are prone to overfitting. The Fitness Function must heavily penalize tree size to ensure the GA doesn't just produce massive trees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}